---
title: "Indexed table"
author: "Ricardo Rodrigo Basa"
date: "2024-01-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In the previous chapter, we saw that an SQL DB parses through the whole table to retrieve
rows. Because it does not know where the rows that match the provided conditions are, it 
has to check every row. This is why it does not matter where in the table the rows are
located. This is where INDEXes come in.

## Duplicate our table

We want to keep our non-indexed table so that we can still run non-indexed queries later.

```{sql, eval=FALSE}
CREATE TABLE `sample_table_indexed`
AS SELECT * FROM sample_table;
```

It takes about 15 minutes to make a copy.

Then we add the primary key and indexes for `fk_id` and `entry_date`.

```{sql, eval=FALSE}
ALTER TABLE `sample_table_indexed`
ADD PRIMARY KEY(`id`),
ADD INDEX `fk_id_idx` (`fk_id`),
ADD INDEX `entry_date_idx` (`entry_date`);
```

It takes about 20 minutes to add these indices.

## Database Connection

In the background, we set up our environment, connect to the database, and turn on profiling.

```{r load_libs, include=FALSE}
library(dplyr)
library(DBI)
library(glue)
library(DT)
```

```{r connect_to_db, include=FALSE}
conn <- dbConnect(
  RMariaDB::MariaDB(),
  host = Sys.getenv("DB_HOST"),
  port = 3306,
  username = Sys.getenv("DB_USER"),
  password = Sys.getenv("DB_PASSWORD"),
  dbname = Sys.getenv("DB_SCHEMA")
)
```

```{r set_profiling, include=FALSE}
dbExecute(conn, "SET @@profiling = 1")
```

A quick check of our tables:

```{r table_check}
dbGetQuery(conn, "DESCRIBE sample_table;")
dbGetQuery(conn, "SHOW INDEXES FROM sample_table;")
dbGetQuery(conn, "DESCRIBE sample_table_indexed;")
dbGetQuery(conn, "SHOW INDEXES FROM sample_table_indexed;")
dbGetQuery(conn, "SELECT COUNT(*) FROM sample_table_indexed;")
```

Our tables are the same except for the indexes.

## Show Profile function

Because we will run profiling repeatedly, it makes sense to write it into a function.

```{r profile_func}
show_profile <- function(conn) {
  this_profile <- dbGetQuery(conn, "SHOW PROFILES") %>%
    slice_tail(n = 3) %>%
    slice_head(n = 1)
  this_query_id <- this_profile %>% pull(Query_ID)
  
  profile <- dbGetQuery(conn, glue("SHOW PROFILE FOR QUERY {this_query_id}"))
  
  print(this_profile)
  print(" ")
  print(profile)
}
```

## Query 6: Run the "first day" query with the benefit of an index

```{r query_6}
result <- dbGetQuery(conn, "SELECT * FROM sample_table_indexed WHERE entry_date = '1973-01-01'")
show_profile(conn)
```

From 17 seconds, we are now down to below 0.007 seconds.

## Query 7: Run the "last day" query with the benefit of an index

```{r query_7}
result <- dbGetQuery(conn, "SELECT * FROM sample_table_indexed WHERE entry_date = '2023-12-31'")
show_profile(conn)
```

It is the same for our "last day" query. Below 0.007 seconds. The DB is not parsing the
entire table anymore.

But, this is a little bit of a cheat. Remember that `entry_date` is already sorted. Rows
with the same `entry_date`s are together. How much difference is there if the needles
are scattered all over the table?

## Query 8: Retrieve all fk_id = 45

We have a different column we can filter on. `fk_id` is not sorted. It is randomly 
distributed across the entire table. Let's run a baseline on the non-indexed table.

```{r query_8}
result <- dbGetQuery(conn, "SELECT * FROM sample_table WHERE fk_id = 45")
result %>% nrow()
show_profile(conn)
```

As expected, we get about 17 seconds.

## Query 9: Retrieve all fk_id = 45 with the help of an index

Then the same query from the indexed table.

```{r query_9}
result <- dbGetQuery(conn, "SELECT * FROM sample_table_indexed WHERE fk_id = 45")
result %>% nrow()
show_profile(conn)
```

We have a substantial improvement from 17 seconds to less than 5 seconds.

## Questions

1. What if we need to filter by year? Or by year-month?

```{r discconect_db, echo=T, results='hide'}
dbExecute(conn, "SET @@profiling = 0")
dbDisconnect(conn)
```
